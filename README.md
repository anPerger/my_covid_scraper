# my_covid_scraper

Web scraper and Flask app:

The final part of our project was to approach the question from a different angle to accomplish something similar but noticably different. Instead of pulling in large amounts of data from CSV files to create a large database of COVID-19 testing data across states, what about trying to scrape just a few selective fields of data off the website that provides the same data? In this case, from the URL https://covidtracking.com/data (the same source as the CSV files from above) using the beautifulsoup and splinter modules, and pulled out just the state name, total cases, and new cases (as updated daily). This data was stored as a list of dictionaries that was then converted into a Mongo database and used for a flask app to display the scraped data in a bootstrap HTML table on a localhost server.

This process mirrors the same basic ETL steps taken using CSVs using pandas, just in a noticably different way. The extraction part is obviously the most differnt. Instead of selecting CSVs containing lots of fields that need to be merged into a pandas dataframe and cleaned of unwanted data, scraping the website into a beautifulsoup object provides all the information on the webpage in a format that's virtually impossible to read and needs to be traversed with splinter to pull out the wanted information.

Once this data has been scraped into a beautifulsoup object, the transformation step simply involves traversal to desired data and reformated as a list of dictionaries. In the initial build of the scraper, I was able to use 'class' to pull the data, but after running it later on I discovered that the class names had been changed, which may be an anti-scraping tactic used by the website. I had to rewrite it to use 'browser.find_by_xpath' which now works just as well as the original build.

After the extraction (scraping) and transformation (spliter tranversal and saving as a list of dictionaries), the final step of loading this into a Mongo database happens automatically within the flask app after the scrape has been initiated with the browser redirecting back to the homepage where the scraped data is displayed as a bootstrap table.

This Extraction, Transformation, and Loading process looks very different from the CSVs-to-pandas-dataframe method, and it has it's own benefits, downsides, and challenges. One benefit is that once the app and scraper have been created, it can be used to retrieve all the fields of data specified as it has been updated on the site just by running the scraper again. This makes it very convinient if being used regularly to retrieve the most up-to-date information from the same web source. The biggest downside to this method as opposed to using pandas is that you need to know ahead of time what information you want to get, or else be forced to set up a splinter traversal to every single field of data that you might possibly want. This can get cumbersome and requires spending a lot of time using the inspector function in your webbrowser to fully understand how to get the data you want.

ideally, the Mongo DB would be dropped every time the scrape was run to ensure the data displayed was actually up to date and not leftover from a previous scrape. However, this was not able to be accomplished in time for submission of this porject, and instead the page now displays the date and time of the most recent scrape.
